{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac0c0b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] == os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349c96f5",
   "metadata": {},
   "source": [
    "# Agentic RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "53a46cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Annotated\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI \n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import HumanMessage, SystemMessage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2503541a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of pdf files 3.\n",
      "total pages loaded 3\n",
      "total documents chunked into docs split 9\n"
     ]
    }
   ],
   "source": [
    "pdf_folder = r\"C:\\AgenticAI\\Projects_AgentAI\\AgenticAI_04_companypolicy\\pdf files\"      # where pdf files are stored\n",
    "\n",
    "pdf_files = [os.path.join(pdf_folder, f) for f in os.listdir(pdf_folder) if f.endswith(\".pdf\")]          # collect all PDF files\n",
    "print(f\"len of pdf files {len(pdf_files)}.\")\n",
    "\n",
    "\n",
    "docs = []\n",
    "for file in pdf_files:\n",
    "    loader = PyPDFLoader(file)\n",
    "    docs.extend(loader.load())\n",
    "print(f\"total pages loaded {len(docs)}\")\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs_splitter = text_splitter.split_documents(docs)\n",
    "print(f\"total documents chunked into docs split {len(docs_splitter)}\")\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "## add these text to vectordb\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents = docs_splitter,\n",
    "    embedding = embeddings\n",
    ")\n",
    "\n",
    "## convert vectordb to retreiver\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\":3})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7854a2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_SYSTEM_MESSAGE = \"\"\"\n",
    "You are an expert HR policy assistant.\n",
    "Answer the question strictly based on the provided policy document context.\n",
    "Be concise, factual, and do not add extra assumptions.\n",
    "If the answer is not in the document, say: \"The policy document does not mention this specifically.\n",
    "\"\"\"\n",
    "\n",
    "USER_PROMPT = \"Question: {input_text}\"\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4.1\", temperature=0)\n",
    "\n",
    "\n",
    "def answer_policy_question(question: str):\n",
    "    \"\"\"llm will generate precise answer from pdf documents relevant to User's question\"\"\"\n",
    "        \n",
    "    relevant_docs = retriever.get_relevant_documents(question)\n",
    "\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "\n",
    "    system_text = SystemMessage(content = RAG_SYSTEM_MESSAGE)\n",
    "\n",
    "    user_text = HumanMessage(content=f\"Question: {question}\\n\\nPolicy Document Context :\\n{context}\")\n",
    "\n",
    "    response = llm.invoke([system_text, user_text])\n",
    "\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "049715c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Final Answer: \n",
      "\n",
      "The working hours policy states that employees must work a minimum of 8 hours per day and 40 hours per week unless otherwise approved by HR. Employees are expected to work 8 hours per day, Monday to Friday.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the working hours policy?\"\n",
    "answer = answer_policy_question(question)\n",
    "\n",
    "print(\"\\n Final Answer: \\n\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881bfcc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf2332e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
